

马尔可夫链：一个显著的特征是，下一个状态只与当前状态有关，与过去的状态无关。
$$
h_{t}=\left\{s_{1}, s_{2}, s_{3}, \ldots, s_{t}\right\}
$$

$$
p\left(s_{t+1}=s^{\prime} \mid s_{t}=s\right)
$$

马尔可夫奖励过程（Markov Reward Process）：在马尔可夫链的基础上加上奖励函数。可以视为一个**随波逐流**的小船。
$$
R\left(s_{t}=s\right)=\mathbb{E}\left[r_{t} \mid s_{t}=s\right]
$$
Horizon：一次游戏的最大步数，可以是无限的。

Return：结果，一般指的是 Reward。
$$
G_{t}=R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\gamma^{3} R_{t+4}+\ldots+\gamma^{T-t-1} R_{T}
$$
从上面的公式可以看出，当我们把 gamma 设置成 0 的时候，价值只与立刻获得的奖励有关，当设置为 1 的时候未来的奖励与现在的奖励的重要程度是相同的。
$$
\begin{aligned}
V_{t}(s) &=\mathbb{E}\left[G_{t} \mid s_{t}=s\right] \\
&=\mathbb{E}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\ldots+\gamma^{T-t-1} R_{T} \mid s_{t}=s\right]
\end{aligned}
$$
在马尔可夫奖励过程中，给定概率矩阵 P 以及价值函数 R，如何计算某一状态的价值。

（未验证）可以设置一个向量，向量的大小和状态的大小相当。然后用向量乘以概率矩阵就可以得到每一步后状态的分布，将状态的分布乘以奖励就可以计算出每一步的奖励。将每一步的奖励打折然后相加就可以得到状态的价值。
$$
V(s)=R(s)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s\right) V\left(s^{\prime}\right)
$$
上面的等式表达的含义是，一个状态的价值等于这个状态的奖励加上转移后的状态的价值。注意上述的R(S)可以表示的是下一个状态的奖励。

Monte Carlo 方法是随机产生轨迹，然后打折相加得到状态的价值。

动态规划的方法是根据 Bellman Equation 迭代计算出状态的价值。

马尔可夫决策过程在马尔可夫奖励过程的基础之上增加了自主的决策过程。

